{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import allin1\n",
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.data.audio import audio_write\n",
    "from pydub import AudioSegment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can analyze a single file:\n",
    "result = allin1.analyze('.//assets//empty Brain.mp3')\n",
    "\n",
    "# Or multiple files:\n",
    "#results = allin1.analyze(['your_audio_file1.wav', 'your_audio_file2.mp3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Audio from sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\dev\\audiocraft\\.venv\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-melody')\n",
    "model.set_generation_params(duration=8)  # generate 8 seconds.\n",
    "\n",
    "# Generate unconditional audio samples\n",
    "wav_unconditional = model.generate_unconditional(4)  # generates 4 unconditional audio samples\n",
    "\n",
    "# Generate audio samples based on descriptions\n",
    "descriptions = ['breakcore', 'IDM', 'hyperpop']\n",
    "wav_descriptions = model.generate(descriptions)  # generates 3 samples.\n",
    "\n",
    "# Load melody and generate audio samples with chroma\n",
    "melody, sr = torchaudio.load('.//assets//trimmed_audiofile.mp3')\n",
    "wav_chroma = model.generate_with_chroma(descriptions, melody[None].expand(3, -1, -1), sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitch audio segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert tensor to AudioSegment\n",
    "def tensor_to_audiosegment(tensor, sample_rate):\n",
    "    audio_array = tensor.cpu().numpy()\n",
    "    audio_array = (audio_array * 32767).astype(np.int16)  # Convert to int16\n",
    "    return AudioSegment(\n",
    "        audio_array.tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=audio_array.dtype.itemsize,    \n",
    "        channels=1\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert generated audio tensors to AudioSegment\n",
    "audio_segments = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='blended_output.wav'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Process chroma-based audio samples\n",
    "for wav in wav_chroma:\n",
    "    audio_segments.append(tensor_to_audiosegment(wav, model.sample_rate))\n",
    "\n",
    "# Concatenate all audio segments\n",
    "blended_audio = sum(audio_segments)\n",
    "\n",
    "# Export the final blended audio\n",
    "blended_audio.export(\"blended_output.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process unconditional audio samples\n",
    "for wav in wav_unconditional:\n",
    "    audio_segments.append(tensor_to_audiosegment(wav, model.sample_rate))\n",
    "\n",
    "# Process description-based audio samples\n",
    "for wav in wav_descriptions:\n",
    "    audio_segments.append(tensor_to_audiosegment(wav, model.sample_rate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
